{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        README:\n",
    "\n",
    "                                      Instructions:\n",
    "\n",
    "1) For the first prompt, please respond ‘wall street journal’ or ‘Wall Street Journal’.\n",
    "\n",
    "2) For the second prompt, please input a keyword you are interested in. \n",
    "\n",
    "3) For the third prompt, please enter a START date in the form of YYYY-MM-DD.\n",
    "\n",
    "4) For the fourth prompt, please enter a END date in the form of YYYY-MM-DD.\n",
    "\n",
    "5) For the fifth prompt, asked after the dataframes are computed, please answer ‘A’, ‘B’, or ‘C’ according to the prompt. \n",
    "\n",
    "It is normal for the code to take some time to compute the dataframes (it took 40 seconds for my computer to compute 1 month worth of ‘trump’ articles, which ended up being 913 articles.  I took more than a minute to compute 1 year’s worth of ‘egg’ articles). I advise you to set a time interval that is close to one another  (>1 year), especially with topics that might yield many results (ex. ‘trump’). Popular topics, such as ‘trump’, are so densely populated that they will yield an extreme amount of articles, which will take a very long time to compute. My computer times out (or reaches the return limit) after around 1500 articles. \n",
    "\n",
    "It is a good idea to restart the Kernel after each run. \n",
    "\n",
    "Also, please keep in mind that my code does not account for duplicates, or the re-publication of the same article in different subsections (ex. Travel, Business, etc.). This might cause some dates to look out of place in bigger (multi-year) datasets, and is explained more in the document below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4\n",
    "import lxml\n",
    "import re\n",
    "import datetime\n",
    "import pprint\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_report():\n",
    "    print('Welcome to the Bias Report.')\n",
    "    newspaper=input('Enter Newspaper Name:')   #ask for user inputs\n",
    "    keyword=input('Enter a Keyword:')\n",
    "    time1=input('Enter START date (YYYY/MM/DD):')\n",
    "    time2=input('Enter END date (YYYY/MM/DD):')\n",
    "    if ('wall street'in newspaper) or ('Wall Street' in newspaper):\n",
    "        url_temp='https://www.wsj.com/search/term.html?KEYWORDS='+keyword+'&min-date='+time1+'&max-date='+time2+'&daysback=4y&isAdvanced=true&andor=AND&sort=date-desc&source=wsjarticle,wsjblogs,wsjvideo,interactivemedia,sitesearch,wsjpro&page='+'1'\n",
    "        dict_title,dict_summary,construction,limit=wsj_bias(url_temp,keyword,time1,time2,'1',0,0,100)\n",
    "        sentiment,sentiment_titles,sentiment_summaries=sentiment_analyzer(dict_summary)\n",
    "        graph_bias(sentiment,sentiment_titles,sentiment_summaries,dict_title,construction)\n",
    "    else:\n",
    "        print('Please input \"wall street journal\" or \"Wall Street Journal\" as the newspaper. If confused, look at instructions.')\n",
    "        \n",
    "        \n",
    "        \n",
    "               \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(summary_library):     #sentiment analysis tool using TextBlob\n",
    "\n",
    "    keyview_summary=summary_library.keys\n",
    "    sentiment_titles={}\n",
    "    sentiment_summaries={}\n",
    "    sentiment={}\n",
    "\n",
    "    for k in keyview_summary():    \n",
    "        summary=summary_library.get(k)\n",
    "        score_title=TextBlob(k) \n",
    "        score_summary=TextBlob(summary)\n",
    "        final_score=(score_title.sentiment.polarity +  score_summary.sentiment.polarity)/2\n",
    "        sentiment[k]=final_score\n",
    "        sentiment_titles[k]=score_title.sentiment.polarity\n",
    "        sentiment_summaries[k]=score_summary.sentiment.polarity\n",
    "\n",
    "    return sentiment, sentiment_titles, sentiment_summaries  #returns sentiment scores of net (title+sumary/2), only title scores and only summary scores as dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_bias(sentiment,sentiment_titles,sentiment_summaries,dict_title,construction):        ###outputs plotly graph \n",
    "    \n",
    "    dates=[]\n",
    "    sentiment_final=[] \n",
    "    sentiment_title_final = []\n",
    "    sentiment_summary_final = []\n",
    "    article_name=[]\n",
    "    \n",
    "    pos_ct_net=0    #these will be used to count each negative, positive and neutral score\n",
    "    neg_ct_net=0\n",
    "    net_ct_net=0\n",
    "    pos_ct_tit=0\n",
    "    neg_ct_tit=0\n",
    "    net_ct_tit=0\n",
    "    pos_ct_sent=0\n",
    "    neg_ct_sent=0\n",
    "    net_ct_sent=0\n",
    "    \n",
    "    sentiment_keys=sentiment.keys()\n",
    "    dict_title_keys=dict_title.keys()\n",
    "    \n",
    "    for m in sentiment_keys:        #extract sentiment score data from dictionaries\n",
    "        value_sentiment=sentiment.get(m)\n",
    "        sentiment_final = sentiment_final + [value_sentiment]\n",
    "        if value_sentiment>0:\n",
    "            pos_ct_net=pos_ct_net+1\n",
    "        elif value_sentiment<0:\n",
    "            neg_ct_net=neg_ct_net+1\n",
    "        else:\n",
    "            net_ct_net=net_ct_net+1\n",
    "            \n",
    "        \n",
    "    for i in dict_title_keys:        #extract date data from dictionaries\n",
    "        article_name= article_name + [i]\n",
    "        date=dict_title.get(i)\n",
    "        dates= dates + [date]\n",
    "        \n",
    "    sentiment_keys=list(sentiment_keys)\n",
    "    index=dates\n",
    "        \n",
    "    for k in sentiment_titles:        #extract sentiment score data from dictionaries\n",
    "        value_title=sentiment_titles.get(k)\n",
    "        sentiment_title_final = sentiment_title_final + [value_title]\n",
    "        if value_title>0:\n",
    "            pos_ct_tit=pos_ct_tit+1\n",
    "        elif value_title<0:\n",
    "            neg_ct_tit=neg_ct_tit+1\n",
    "        else:\n",
    "            net_ct_tit=net_ct_tit+1    \n",
    "\n",
    "    for j in sentiment_summaries:        #extract sentiment score data from dictionaries\n",
    "        value_summary=sentiment_summaries.get(j)\n",
    "        sentiment_summary_final = sentiment_summary_final + [value_summary]\n",
    "        if value_summary>0:\n",
    "            pos_ct_sent=pos_ct_sent+1\n",
    "        elif value_summary<0:\n",
    "            neg_ct_sent=neg_ct_sent+1\n",
    "        else:\n",
    "            net_ct_sent=net_ct_sent+1  \n",
    "            \n",
    "    df = pd.DataFrame(sentiment_keys, index = dates, columns = ['Title'])\n",
    "    df['Net Sentiment']=sentiment_final\n",
    "    df['Title Score']=sentiment_title_final\n",
    "    df['Summary Score']=sentiment_summary_final\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    ans=input('Would you like a graph of the (A) Title and Summary Average Sentiment (A) , (B) Only Title Sentiment (B), or (C) Only Summary Sentiment (C) ?')\n",
    "    length=len(sentiment_keys)\n",
    "    \n",
    "    #the formatting of the graph (title, axis names, hover formatting) were taken from plotly's online manual,\n",
    "    #on sites such as https://plot.ly/python/axes/, https://plot.ly/python/hover-text-and-formatting/,\n",
    "    #https://plot.ly/python/text-and-annotations/, https://plot.ly/python/line-and-scatter/.\n",
    "    \n",
    "    \n",
    "    if ans is 'A': #want average title and summary score\n",
    "        p1 = px.scatter(x=index, y=sentiment_final, hover_name=article_name)\n",
    "        p1.update_yaxes(range=[-1,1])\n",
    "        p1.update_layout(\n",
    "            title=\"Title and Summary Combined Sentiment\", \n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Combined Sentiment\",\n",
    "            font=dict(\n",
    "                family=\"Times New Roman, monospace\",\n",
    "                size=18,\n",
    "                color=\"#d62728\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        p1.show()\n",
    "        \n",
    "        print('In',length,'articles,',pos_ct_net,'are positive,',neg_ct_net,'are negative, and',net_ct_net,'are neutral. ',construction,'articles are being updated.')\n",
    "\n",
    "    elif ans is 'B': #want only title score\n",
    "        p2 = px.scatter(x=index, y=sentiment_title_final, hover_name=article_name)\n",
    "        p2.update_yaxes(range=[-1,1])\n",
    "        \n",
    "        p2.update_layout(\n",
    "            title=\"Title Sentiment\", \n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Title Sentiment\",\n",
    "            font=dict(\n",
    "                family=\"Times New Roman, monospace\",\n",
    "                size=18,\n",
    "                color=\"#d62728\"\n",
    "            )\n",
    "        )\n",
    "        p2.show()\n",
    "        \n",
    "        construction=int(construction)\n",
    "        construction=construction-5\n",
    "        construction=str(construction)\n",
    "        \n",
    "        print('In',length,'articles,',pos_ct_tit,'are positive,',neg_ct_tit,'are negative, and',net_ct_tit,'are neutral. ',construction,'articles are being updated.')\n",
    "\n",
    "    elif ans is 'C':   #want only summary score\n",
    "        p3 = px.scatter(x=index, y=sentiment_summary_final, hover_name=article_name)\n",
    "        p3.update_yaxes(range=[-1,1])\n",
    "        \n",
    "        p3.update_layout(\n",
    "            title=\"Summary Sentiment\", \n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Summary Sentiment\",\n",
    "            font=dict(\n",
    "                family=\"Times New Roman, monospace\",\n",
    "                size=18,\n",
    "                color=\"#d62728\"\n",
    "            )\n",
    "        )      \n",
    "        \n",
    "        p3.show()\n",
    "        \n",
    "        print('In',length,'articles,',pos_ct_sent,'are positive,',neg_ct_sent,'are negative, and',net_ct_sent,'are neutral. ',construction,'page(s) are being updated.')\n",
    "\n",
    "    else:\n",
    "        print('Try again! Please only use CAPITAL A, B or C.')\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsj_bias(url_temp,keyword,time1,time2,pagenum_str,construction,counts,limit):\n",
    "    import urllib.request\n",
    "    import bs4\n",
    "    import lxml\n",
    "    import re\n",
    "    import datetime\n",
    "    import pprint\n",
    "    \n",
    "    wsj={}\n",
    "    wsj_summary={}\n",
    "    today=datetime.datetime.now()\n",
    "    final_date={}\n",
    "    final_summary={}\n",
    "    limit_wsj=[]\n",
    "    \n",
    "    url_wsj = urllib.request.urlopen(url_temp)     #put url through BeautifulSoup\n",
    "    soup_wsj = bs4.BeautifulSoup(url_wsj, 'lxml', from_encoding='utf-8')\n",
    "    headline_container_wsj=soup_wsj.findAll('div',attrs={\"class\":\"headline-container\"}) #find headline containers\n",
    "    limit_wsj=soup_wsj.findAll('li',attrs={\"class\":\"results-count\"})\n",
    "    \n",
    "\n",
    "    limit_wsj=[k.text for k in limit_wsj]    \n",
    "    limit_wsj=[int(k[(k.find('of'))+3:]) for k in limit_wsj]\n",
    "    \n",
    "    if len(limit_wsj)==2:\n",
    "        if limit_wsj[0]>limit_wsj[1]:\n",
    "            limit=limit_wsj[1]\n",
    "        else:\n",
    "            limit=limit_wsj[0]\n",
    "            \n",
    "    for k in headline_container_wsj:           \n",
    "        title=k.find('h3',attrs={\"class\":\"headline\"})   #find everything else in the container\n",
    "        title=title.text\n",
    "        date=k.find('time',attrs={\"class\":\"date-stamp-container highlight\",\"class\":\"date-stamp-container\"})\n",
    "        date=date.text\n",
    "        summary=k.find('div',attrs={\"class\":\"summary-container\"})\n",
    "        wsj[title]=date\n",
    "        try:\n",
    "            summary=summary.text\n",
    "            wsj_summary[title]=summary\n",
    "        except:\n",
    "            wsj_summary[title]='n/a'\n",
    "\n",
    "    \n",
    "    keyview_wsj=wsj.keys\n",
    "    \n",
    "    for k in keyview_wsj():    #formatting time, as WSJ has a lot of inconsistencies\n",
    "        val=wsj.get(k)         #when reporting the time an article is published\n",
    "        \n",
    "        if 'Just' in val:\n",
    "            val=val[5:]\n",
    "    \n",
    "        if 'Jan.' in val:                           #standardizing months as they abbreviate\n",
    "            val=val.replace('Jan.','January')       #sometimes and do not other times\n",
    "        elif 'Feb.' in val:\n",
    "            val=val.replace('Feb.','February')\n",
    "        elif 'Mar.' in val:\n",
    "            val=val.replace('Mar.','March')    \n",
    "        elif 'Apr.' in val:\n",
    "            val=val.replace('Apr.','April')\n",
    "        elif 'Jun.' in val:\n",
    "            val=val.replace('Jun.','June')\n",
    "        elif 'Jul.' in val:\n",
    "            val=val.replace('Jul.','July')\n",
    "        elif 'Aug.' in val: \n",
    "            val=val.replace('Aug.','August')\n",
    "        elif 'Sep.' in val:    \n",
    "            val=val.replace('Sep.','September')\n",
    "        elif 'Oct.' in val:  \n",
    "            val=val.replace('Oct.','October')\n",
    "        elif 'Nov.' in val: \n",
    "            val=val.replace('Nov.','November')\n",
    "        elif 'Dec.' in val: \n",
    "            val=val.replace('Dec.','December')\n",
    "            \n",
    "    \n",
    "        if 'hour' in val:                      #standardizing dates reported in hrs or mins\n",
    "            number=''                          #into YYYY-MM-DD format\n",
    "            for m in val:\n",
    "                if re.match('\\d',m):\n",
    "                    number=number+m\n",
    "            number=int(number)\n",
    "            new_date= today + datetime.timedelta(hours = -number)\n",
    "            wsj[k]=new_date.strftime('%Y-%m-%d')\n",
    "            val=datetime.datetime.strptime(wsj[k], '%Y-%m-%d')\n",
    "            \n",
    "        elif 'min' in val:\n",
    "            number=''\n",
    "            for n in val:\n",
    "                if re.match('\\d',n):\n",
    "                    number=number+n\n",
    "            number=int(number)\n",
    "            new_date= today + datetime.timedelta(minutes = -number)\n",
    "            wsj[k]=new_date.strftime('%Y-%m-%d')\n",
    "            val=datetime.datetime.strptime(wsj[k], '%Y-%m-%d')\n",
    "\n",
    "        \n",
    "        else:\n",
    "            val=val[0:-2]                     #getting rid of time zone \n",
    "            val=datetime.datetime.strptime(val, '%B %d, %Y %I:%M %p ') \n",
    "            wsj[k]=val.strftime('%Y-%m-%d')\n",
    "            val=datetime.datetime.strptime(wsj[k], '%Y-%m-%d')  #change back to compare with datetime objects if wanted\n",
    "    \n",
    "        final_date[k]=wsj[k]\n",
    "        summary_value=wsj_summary[k]\n",
    "        final_summary[k]=summary_value\n",
    "        \n",
    "    if len(wsj)==0:\n",
    "        construction=construction+1 \n",
    "    \n",
    "    counts=counts+1\n",
    "    \n",
    "    print('Page',counts,'/',limit,'done.')\n",
    "    \n",
    "    if counts<=limit:\n",
    "        pagenum=int(pagenum_str)\n",
    "        pagenum=pagenum+1\n",
    "        pagenum_str=str(pagenum)\n",
    "        url_temp='https://www.wsj.com/search/term.html?KEYWORDS='+keyword+'&min-date='+time1+'&max-date='+time2+'&daysback=4y&isAdvanced=true&andor=AND&sort=date-desc&source=wsjarticle,wsjblogs,wsjvideo,interactivemedia,sitesearch,wsjpro&page='+pagenum_str\n",
    "        add_date,add_summary,construction,limit=wsj_bias(url_temp,keyword,time1,time2,pagenum_str,construction,counts,limit) \n",
    "                  \n",
    "        final_date={**wsj, **add_date}  ####### combine dicts\n",
    "        final_summary={**wsj_summary, **add_summary}\n",
    "    \n",
    "    return final_date,final_summary, construction, limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Bias Report.\n",
      "Enter Newspaper Name:wall street\n",
      "Enter a Keyword:trump\n",
      "Enter START date (YYYY/MM/DD):2019/01/01\n",
      "Enter END date (YYYY/MM/DD):2019/01/11\n",
      "Page 1 / 16 done.\n",
      "Page 2 / 16 done.\n",
      "Page 3 / 16 done.\n",
      "Page 4 / 16 done.\n",
      "Page 5 / 16 done.\n"
     ]
    }
   ],
   "source": [
    "bias_report()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
